from torch.utils.data import Subset
from PIL import Image
from torchvision.datasets import CIFAR100
from base.torchvision_dataset import TorchvisionDataset
from .preprocessing import get_target_label_idx, global_contrast_normalization

import torchvision.transforms as transforms


class CIFAR100_Dataset(TorchvisionDataset):

    def __init__(self, root: str, normal_class=5):
        super().__init__(root)

        self.n_classes = 2  # 0: normal, 1: outlier
        self.normal_classes = tuple([normal_class])
        self.outlier_classes = list(range(0, 10))
        self.outlier_classes.remove(normal_class)

        # Pre-computed min and max values (after applying GCN) from train data per class
        min_max = [
           (-5.948405742645264 , 9.660704612731934),
(-10.135717391967773 , 10.288029670715332),
(-8.010988235473633 , 6.159584045410156),
(-7.860760688781738 , 6.403110027313232),
(-6.162306308746338 , 12.356056213378906),
(-6.718392848968506 , 7.207522869110107),
(-7.716285228729248 , 7.625071525573730),
(-13.258931159973145 , 9.566340446472168),
(-7.180139064788818 , 8.793897628784180),
(-10.873169898986816 , 9.632754325866699),
(-8.865124702453613 , 10.648329734802246),
(-7.586984634399414 , 7.477134227752686),
(-5.724575042724609 , 7.453512191772461),
(-4.049036502838135 , 8.111802101135254),
(-7.554550170898438 , 7.613654613494873),
(-7.047618389129639 , 7.777510643005371),
(-6.843384265899658 , 11.542062759399414),
(-5.170035839080811 , 7.548356533050537),
(-12.173405647277832 , 7.535705566406250),
(-7.734030723571777 , 8.899975776672363),
(-14.170866966247559 , 5.102953910827637),
(-3.948725223541260 , 10.450243949890137),
(-10.165564537048340 , 14.258680343627930),
(-9.781021118164062 , 5.949658870697021),
(-15.715434074401855 , 5.180377483367920),
(-7.689728260040283 , 10.851978302001953),
(-13.568424224853516 , 10.576196670532227),
(-8.677091598510742 , 8.298542976379395),
(-11.916910171508789 , 14.995089530944824),
(-7.343737602233887 , 10.493697166442871),
(-10.411479949951172 , 9.556527137756348),
(-4.927999019622803 , 8.111904144287109),
(-10.003246307373047 , 7.759814739227295),
(-5.247570037841797 , 8.556865692138672),
(-7.686244487762451 , 7.697745323181152),
(-8.212591171264648 , 7.712148666381836),
(-6.724447250366211 , 6.806134223937988),
(-5.371448516845703 , 7.020014286041260),
(-6.864624977111816 , 7.626443386077881),
(-6.658708572387695 , 8.918777465820312),
(-13.702791213989258 , 14.160848617553711),
(-9.808536529541016 , 7.488389968872070),
(-6.019726276397705 , 8.843354225158691),
(-5.875588893890381 , 6.852039337158203),
(-12.311223983764648 , 8.643749237060547),
(-10.768772125244141 , 10.643512725830078),
(-6.467815876007080 , 10.639573097229004),
(-4.407519817352295 , 6.769673347473145),
(-6.956187725067139 , 6.522030830383301),
(-6.451095581054688 , 6.703621387481689),
(-7.745534896850586 , 7.141514301300049),
(-5.182530403137207 , 8.843543052673340),
(-3.271659612655640 , 8.343873023986816),
(-5.630589962005615 , 6.021442413330078),
(-10.161286354064941 , 6.834056377410889),
(-14.475379943847656 , 12.727408409118652),
(-6.648282527923584 , 6.971134662628174),
(-9.367555618286133 , 10.220548629760742),
(-5.368873119354248 , 7.251843929290771),
(-6.102238178253174 , 8.462959289550781),
(-7.543046474456787 , 5.360929489135742),
(-19.340866088867188 , 6.020494937896729),
(-4.842134475708008 , 14.850487709045410),
(-4.713560104370117 , 6.903638362884521),
(-9.084371566772461 , 8.979685783386230),
(-7.541802406311035 , 7.840998172760010),
(-5.826699733734131 , 9.353659629821777),
(-7.785328865051270 , 9.473778724670410),
(-5.694646358489990 , 9.882837295532227),
(-8.545856475830078 , 10.469579696655273),
(-6.972451686859131 , 8.089874267578125),
(-6.941705703735352 , 7.579060077667236),
(-10.701529502868652 , 10.832575798034668),
(-7.436810493469238 , 10.745798110961914),
(-7.605423450469971 , 10.357999801635742),
(-10.214243888854980 , 8.064427375793457),
(-5.972078800201416 , 10.153846740722656),
(-9.706583023071289 , 8.867867469787598),
(-9.958529472351074 , 8.747625350952148),
(-18.856040954589844 , 20.569698333740234),
(-7.833275794982910 , 9.763311386108398),
(-4.488684177398682 , 7.981661319732666),
(-6.518854618072510 , 7.365106582641602),
(-8.000015258789062 , 12.205577850341797),
(-10.153443336486816 , 7.189603328704834),
(-4.712369918823242 , 8.339100837707520),
(-10.818424224853516 , 7.576972007751465),
(-5.915587902069092 , 7.568691730499268),
(-4.616147994995117 , 6.864666461944580),
(-5.234558582305908 , 6.933371543884277),
(-4.588479042053223 , 11.975707054138184),
(-7.516494274139404 , 12.160161018371582),
(-6.772746086120605 , 13.167462348937988),
(-5.615454673767090 , 8.413414001464844),
(-7.039467334747314 , 8.784221649169922),
(-6.447991371154785 , 7.703426361083984),
(-4.915009498596191 , 11.645584106445312),
(-6.376648902893066 , 7.681932449340820),
(-5.522876739501953 , 8.646308898925781),
(-13.738019943237305 , 21.160188674926758)
                   ]

        # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]
        transform = transforms.Compose([transforms.ToTensor(),
                                        transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),
                                        transforms.Normalize([min_max[normal_class][0]] * 3,
                                                             [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])

        target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))

        train_set = MyCIFAR100(root=self.root, train=True, download=True,
                              transform=transform, target_transform=target_transform)
        # Subset train set to normal class
        train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)
        self.train_set = Subset(train_set, train_idx_normal)

        self.test_set = MyCIFAR100(root=self.root, train=False, download=True,
                                  transform=transform, target_transform=target_transform)


class MyCIFAR100(CIFAR100):
    """Torchvision CIFAR100 class with patch of __getitem__ method to also return the index of a data sample."""

    def __init__(self, *args, **kwargs):
        super(MyCIFAR100, self).__init__(*args, **kwargs)

    def __getitem__(self, index):
        """Override the original method of the CIFAR100 class.
        Args:
            index (int): Index
        Returns:
            triple: (image, target, index) where target is index of the target class.
        """
        if self.train:
            img, target = self.train_data[index], self.train_labels[index]
        else:
            img, target = self.test_data[index], self.test_labels[index]

        # doing this so that it is consistent with all other datasets
        # to return a PIL Image
        img = Image.fromarray(img)

        if self.transform is not None:
            img = self.transform(img)

        if self.target_transform is not None:
            target = self.target_transform(target)

        return img, target, index  # only line changed
